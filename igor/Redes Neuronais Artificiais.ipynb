{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Redes Neuronais Aritifiais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução\n",
    "\n",
    "Redes neuronais são baseadas em um paradigma conexionista ao invés do simbólico (introduzido por [Von Neumann](https://pt.wikipedia.org/wiki/John_von_Neumann)) utilizado pela maioria das linguagens computacionais formais. No modelo simbólico são utilizados predicados que quando processados sequencialmente dão instruções exatas em como um determinado processo deve ser executado, por um outro lado no [paradigma conexionista](https://pt.wikipedia.org/wiki/Conexionismo \"Wikipedia: Conexionismo\") o processamento é feito através de redes interconectadas que são individualmente simples.\n",
    "\n",
    "![Rede de Neurônios](rede_neuronio_bio.jpg \"Rede de Neurônios\")\n",
    "\n",
    "O modelo de inspiração para a criação das redes neuronais é o cérebro humano que é capaz de resolver problemas relacionados com *identificação de padrões* de forma extremamente eficiente e rápida. Este modelo (como modelo que é) não tem objetivo de ser a representação real do modelo biológico, mas sim uma visão simplificada deste sistema que tem como entidade elementar o neurônio. Desta forma temos a visão do *comportamento coletivo* da rede de células.\n",
    "\n",
    "![Esturutra do Neuronio](estrutura-neuronio.jpg \"Estrutura do Neurônio\")\n",
    "\n",
    "O cérebro humano contém uma quantidade enorme de neurônios $ 10^{11} $, cada neurônio é conectado a outros em uma ligação do *dentrito* com o *terminal do axônio*. O sinal elétrico recebido pelos dentritos de um ou mais neurônios é processado no corpo celular através de reações bioquímicas se esta reação é significativa o suficiente o pulso é repassado para o axônio que propaga este sinal para outros neurônios conectados no terminal desta célula.\n",
    "\n",
    "De forma simplificada temos o modelo conhecido como **Neurônio de McCulloch-Pitts**, onde cada entrada $I_n$ tem um peso $W_n$ que pode ser positivo (um estímulo) ou negativo (uma inibição), é então somado e passa por uma função limiar (threshold), caso a soma seja maior que este limiar esta informação é propagada na saída (imagem da função).\n",
    "\n",
    "![Neurônio de McCulloch-Pitts](mcculloch_pits_neuron.png \"Neurônio de McCulloch-Pitts\")\n",
    "\n",
    "Representamos este entidade através da seguinte equação para um dado neurônio $i$:\n",
    "\n",
    "\\begin{equation}\\label{eq:nmcp_discreto}\n",
    "n_i (t+1)  = \\Theta \\left( \\sum_j W_{ij} I_j(t) - T_i \\right) \\; \\; \\; \\text{ onde } 0 < j < N \\in \\mathbb{N}\n",
    "\\end{equation}\n",
    "\n",
    "Neste caso $n_i(t)$ pode ser $0$ ou $1$ e representa o estado do neurônio entre *passando* ou *não-passando* o determinado sinal. O tempo $t$ é discreto e $\\Theta(x)$ é a função [Heaviside](https://pt.wikipedia.org/wiki/Fun%C3%A7%C3%A3o_de_Heaviside).\n",
    "\n",
    "Já foi demonstrado (onde?) que uma ordenação sincrona de neurônios é capaz de fornecer o [princípio de computação universal](https://pt.wikipedia.org/wiki/M%C3%A1quina_de_Turing_universal) para pesos $w_{ij}$ escolhidos arbitrariamente.\n",
    "\n",
    "As principais diferenças entre um neurônio biológico e o de McCulloch-Pitts são que, os reais normalmente não se asemelham a um dispositivo de limite, ao invés disso respondem de forma contínua (**resposta graduada**), por outro lado a hipótese do autor é que ponto tido como essencial para a simulação da rede neuronal é a **não linearidade** que é mantida no modelo simplista baseado em limites.\n",
    "\n",
    "Outro ponto é que as células realizam uma soma *não linear*, permitindo fazer algo similar a operações lógicas, esta estrutura é possível de ser simulada utilizando mais níveis em um neurônio McCulloch-Pitts.\n",
    "\n",
    "Neurônios biológicos são assíncronos e respondem o estímulo gerando uma sequência de pulsos ao invés de um único pulso \"binário\". A transmissão assíncrona é tratada no Neurônio de McCulloch-Pitts (NMCP de agora em diante), por outro lado muitos especialistas acreditam que a fase do pulso não tem papel fundamental (mas não de aceitação geral).\n",
    "\n",
    "A generalização simplista que contém estas caracteristias é dada pela equação:\n",
    "\n",
    "\\begin{equation}\\label{eq:nmcp_continuo}\n",
    "n_i := g \\left( \\sum_j W_{ij} n_j - T_i  \\right)\n",
    "\\end{equation}\n",
    "\n",
    "Aqui a função limite $\\Theta(x)$ foi substituída por uma função contínua e não linear $g(x)$ conhecida como **funcão de transferência**.\n",
    "\n",
    "O cérebro atua então como um sistema de processamento paralelo extremamente eficiente no quesito de paralelismo. Onde cada processador realiza uma operação simples que é a soma das entradas aplicadas seu peso e da como saída um número único, uma função **não-linear**. Podemos pensar nestes pesos como dados armazenados pelos processadores.\n",
    "\n",
    "Este alto paralelismo, mostra que a soma possuirá muitos termos, significando que erros de poucas entradas tem **pouco impacto** de uma forma mais geral.\n",
    "\n",
    "Devemos lembrar que o tempo de resposta da rede neuronal biológica é da ordem de milissegundos enquanto de um computador clássico é da ordem de pico segundos, ainda assim, o cérebro humano é capaz de executar muitas tarefas de forma muito mais eficiente do que um computador como reconhecimento de imagens, identificação de dados ruidosos e controle motor.\n",
    "\n",
    "## Para pensar\n",
    "\n",
    "Inicialmente pode-se pensar (eu pensei) que o paralelismo levado as últimas consequências é a *panacea*, mas antes perguntas devem ser feitas e respondidas, como:\n",
    "\n",
    "* Quantas camadas e conexões devem ser feitas?\n",
    "* Quais são funções ideais para ser usadas como função de transferência $g(x)$?\n",
    "* A rede pode ser treinada?\n",
    "* Como treinar a rede?\n",
    "* Usar um modelo síncrono ou assíncrono?\n",
    "* Quão rápida é a rede para cada uma das tarefas dadas?\n",
    "* É robusta? Suporta falhas ou perda de informação?\n",
    "* Podemos generalizar tarefas?\n",
    "* Que tipo de informações pode representar?\n",
    "* Pode ser construída com as ferramentas existentes?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelo Hopfield\n",
    "\n",
    "## Problema da memória associativa\n",
    "\n",
    "O problema mais elementar é dado por:\n",
    "\n",
    "> Armazenado um conjunto de $p$ padrões $\\xi^{\\mu}$ de uma forma que quando apresentado a um novo padrão $\\zeta_0$, a rede deve responder com o padrão que mais se assemelha à $\\zeta_0$ dos padrões armazenados.\n",
    "\n",
    "Numeramos cada um dos padrões com inteiros $\\mu = 1, 2, \\ldots, p$, enquanto as unidades (células) são numeradas com $i = 1,2, \\ldots, N$. Ambos $\\xi^{\\mu}$ (padrões armazenados) e os padrões de teste $\\zeta^\\nu$ podem ser dados como $0$ ou $1$ em cada local $i$ representados individualmente como $\\xi_i^{\\mu}$ e $\\zeta_i^{\\nu}$.\n",
    "\n",
    "Assim um padrao armazenado $\\xi^{\\mu}$ e' formado por $\\xi^{\\mu} = \\left( \\xi^{\\mu}_1, \\xi^{\\mu}_2, \\ldots, \\xi^{\\mu}_p \\right)$ interpretado como um vetor e representamos o i-esimo elemento o $\\mu$-esimo padrao como $\\xi^{\\mu}_i$. O mesmo se da com $\\zeta^{\\nu}$.\n",
    "\n",
    "\n",
    "Para implementar isso em um computador convencional utilizamos a lista de padrões e escrevemos um programa que calcula a [distância de Hamming](https://pt.wikipedia.org/wiki/Dist%C3%A2ncia_de_Hamming). Matematicamente teríamos:\n",
    "\n",
    "\\begin{equation}\\label{eq:dist_hamming}\n",
    "H(\\xi^{\\mu}) = \\sum_i \\left[ \\xi_i^{\\mu} (1 - \\zeta_i) + (1- \\xi_i^{\\mu}) \\zeta_i \\right]\n",
    "\\end{equation}\n",
    "\n",
    "Como por definição nossos padrões são somente $0$ ou $1$, a função soma incrementar em 1 vai realizar somas cada vez que os valores forem diferentes, e incrementar 0 quando forem iguais."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $\\xi$ | $\\zeta$ | $\\xi\\left(1-\\zeta\\right)$ | $\\left(1-\\xi\\right)\\zeta$ | $\\sum$ \n",
    " :---: | :---: | :---: | :---: | :---:\n",
    "  0  |  0  |  0  |  0  |  0 \n",
    "  1  |  0  |  1  |  0  |  1 \n",
    "  0  |  1  |  0  |  1  |  1\n",
    "  1  |  1  |  0 |   0  |  0 \n",
    "\n",
    "\n",
    "Em outras palavras a distância de Hamming é uma distância espacial que mede quanto dois vetores $u$ e $v$ unidimensionais diferem em suas componentes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para fins práticos (e sem preocupação com performance ou eficiência) vamos fazer um exemplo computacional, seja a função de distância Hamming definida por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hamming_norm(u, v):\n",
    "    # A formula exata e' menos eficiente (duas vezes mais lenta)\n",
    "    # np.sum(u*(1-v)+v*(1-u))\n",
    "    return np.sum(u != v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para dois vetores $u$ e $v$ dados por:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=100000\n",
    "u = np.random.randint(2,size=size,dtype=np.int8)\n",
    "v = np.random.randint(2,size=size,dtype=np.int8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos a distancia de Hamming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50050"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hamming_norm(u,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "h": "2"
    }
   },
   "source": [
    "Podemos perceber que o menor valor da desta distância é 0, quando os vetores são idênticos e quanto maior este valor, menos similaridade há entre os dois vetores. (poderíamos normalizar esta valor, dividindo pela norma do vetor, assim obter um valor entre $0$ e $1$). Outra consequência que podemos notar é que caso o número de componentes (dimensões) deste vetores seja muito grande, erros locais terão pouco impacto (estatísticamente) nesta distância."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## O Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja $S^t$ o estado do sistema no instante $t$, fazemos de $S_i^t$ a representacao do estado do $i$-esimo neuronio no instante $t$.\n",
    "\n",
    "Definimos os valores de $S_i$ como $+1$ ativado e $-1$ nao ativado. A dinamica do estado do sistema e' dada por:\n",
    "\n",
    "\\begin{align}\n",
    "h_i & = \\sum_j W_{ij}S_j \\\\\n",
    "S_i & = sgn \\left( h_i - T_i\\right) \\\\ \n",
    "S_i & = sgn \\left( \\sum_j W_{ij} S_j - T_i\\right) \\\\ \n",
    "sgn(x) & = \n",
    "\\begin{cases}\n",
    "+1 \\; x \\ge 0 \\\\\n",
    "-1 \\; x \\lt 0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Sendo $T_i$  o limiar de ativacao da i-esima celula e $h_i$ o estado deste mesmo neuronio em respeito aos $j$-neuronios conectados."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Escolhemos esta definicao ao inves do $n_i$ previamente apresentado que definia $1$ para ativado e $0$ para nao ativado.\n",
    "\n",
    "Desta forma $S_i$ pode ser escrito equivalentemente como:\n",
    "\n",
    "$$ S_i = 2 n_i -1 \\;\\; n_i \\in \\{0,1\\}$$\n",
    "\n",
    "Se $n_i = 0$\n",
    "\n",
    "$$ S_i = 2 \\cdot 0 -1 = -1 $$\n",
    "\n",
    "Se $n_i = 1$\n",
    "\n",
    "$$ S_i = 2 \\cdot 1 - 1 = 1 $$\n",
    "\n",
    "Vamos demonstrar que esta escolha e' equivalente dado um limiar bem definido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demonstracao"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fazendo $x = \\sum_j W_{ij} (2n_j-1) - T_i$. Abrindo o somatorio temos:\n",
    "\n",
    "\\begin{align}\n",
    "\\sum_j W_{ij} (2n_j-1) &- T_i \\\\\n",
    "\\sum_j \\left( 2 n_j W_{ij} -  W_{ij} \\right) & - T_i  \\\\\n",
    "\\sum_j 2 n_j W_{ij} - \\sum_j W_{ij} & - T_i \\\\\n",
    "2 \\sum_j n_j W_{ij} - \\sum_j W_{ij} & - T_i\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substituindo de volta:\n",
    "\n",
    "$$S_i = sgn \\left( 2 \\sum_j n_j W_{ij} - \\sum_j W_{ij} - T_i\n",
    "\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para a funcao $sgn(x)$ temos dois possiveis valores $1$ e $-1$. Para o primeiro caso teriamos obrigatoriamente $x \\ge 0$ desta forma:\n",
    "\n",
    "\\begin{align}\n",
    "2 \\sum_j n_j W_{ij} - \\sum_j W_{ij} - T_i & \\ge 0 \\\\\n",
    "2 \\sum_j n_j W_{ij} - \\sum_j W_{ij} & \\ge T_i  \\\\\n",
    "2 \\sum_j n_j W_{ij} & \\ge \\sum_j W_{ij} +  T_i  \\\\\n",
    "  \\sum_j n_j W_{ij} & \\ge \\frac{1}{2} \\left( \\sum_j W_{ij} + T_i \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desta forma quando $x \\ge 1$ temos:\n",
    "\n",
    "$$\\mu_i = \\frac{1}{2}  \\left( \\sum_j W_{ij} + T_i \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por outro lado quando a $x \\lt 0$ temos:\n",
    "\n",
    "\\begin{align}\n",
    "2 \\sum_j W_{ij} n_j - \\sum_j W_{ij} - T_i & \\lt 0 \\\\\n",
    "2 \\sum_j W_{ij} n_j - \\sum_j W_{ij} & \\lt T_i  \\\\\n",
    "2 \\sum_j W_{ij} n_j & \\lt \\sum_j W_{ij} +  T_i  \\\\\n",
    "  \\sum_j W_{ij} n_j & \\lt \\frac{1}{2} \\left( \\sum_j W_{ij} + T_i \\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alem disso o limiar $T_i$ esta relacionado com $\\mu_i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Em $\\mu_i$ isolamos $T_i$\n",
    "\n",
    "\\begin{align}\n",
    "\\mu_i &= \\frac{1}{2}  \\left( \\sum_j W_{ij} + T_i \\right) \\\\\n",
    "2 \\mu_i &= \\sum_j W_{ij} + T_i \\\\\n",
    "T_i &= 2 \\mu_i - \\sum_j W_{ij}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar podemos considerar o limiar sendo $0$ assim fazemos $T_i=0 \\; \\forall i$. Desta forma ficamos com:\n",
    "\n",
    "$$S_i = sgn \\left( \\sum_j W_{ij} S_j\\right)$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim sendo a equacao de ativacao e representada pelo grafico\n",
    "\n",
    "<center>![Funcao Ativacao](funcaoativ_11.png)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEKCAYAAAASByJ7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fc8c5f3cf60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=np.array([-2,-1,0,1,2])\n",
    "y=np.where(x<=0,-1,1)\n",
    "fig, ax = plt.subplots()\n",
    "plt.step(x,y)\n",
    "plt.xticks(x)\n",
    "plt.yticks(y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('sgn()')\n",
    "for s in  ax.spines.values():\n",
    "    s.set_visible(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tipos de atualizacao\n",
    "\n",
    "As atualizacoes do sistema no modelo de *Hopfield* podem ser *sincronas* ou *assincronas*.\n",
    "\n",
    "**sincrono**: todos os neuronios se atualizam sincronamente, isto e todos os estados do tempo futuro sao baseados no instante anterior. Matematicamente:\n",
    "\n",
    "$$S_i^{t+1} = sgn \\left( \\sum_j W_{ij} S_j^t \\right)$$\n",
    "\n",
    "**assincrono**: as atualizacoes sao feitas desordenadas, onde cada neuronio e' atualizado por vez. Usados em modelos autonomos. Existem duas escolhas possiveis para este modelo.\n",
    "\n",
    " - a cada instante uma unidade aleatoria e' escolhida e atualizada; ou\n",
    " \n",
    " - cada unidade escolhe um intervalo proprio de atualizacao de forma independente.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rede de Hopfield\n",
    "\n",
    "Dada uma rede com $p$ neuronios. A rede *Hopfield* e' uma rede **completamente conexo**, isto e' cada um dos seus elementos esta conectado a todos os outros elementos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergencia e estabilidade \n",
    "\n",
    "Quando nao ha mais atualizacoes dos estados entre instantes de tempos ou seja $S^{t+1}_i = S^t_i$ dizemos que o estado do neuronio encontra-se estabilizado.\n",
    "\n",
    "Ou seja dado um sistema que tenha armazenado padrao $\\xi^1$, ao atribuirmos um valor inicial ao sistema $S^0$ aplicada a equacao do movimento $S_i$, o sistema ira convergir para um (e somente um) dos estados armazenados $\\xi^1$ ou $-\\xi^1$, ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convergencia no modelo de um unico padrao\n",
    "\n",
    "Vamos supor que $\\xi$ e' um atrator, isto e': ele representa um dos estados de estabilidade. Temos entao:\n",
    "\n",
    "$$sgn \\left( \\sum_j^N W_{ij} \\xi_j \\right) = \\xi_j \\;\\; \\forall i$$\n",
    "\n",
    "Fazendo:\n",
    "\n",
    "\\begin{align}\n",
    "W_{ij} & \\propto \\xi_i \\xi_j \\\\\n",
    " & = b \\xi_i \\xi_j\n",
    "\\end{align}\n",
    "\n",
    "Substituindo temos:\n",
    "\n",
    "$$sgn \\left( \\sum_j^N b \\xi_i \\xi_j \\xi_j \\right) = \\xi_j$$\n",
    "\n",
    "Como $\\xi_j^2 = 1$. Pois $1^2 = \\left(-1\\right)^2 = 1.$\n",
    "\n",
    "\\begin{align}\n",
    "sgn \\left( \\sum_j^N b \\xi_i \\right) = \\xi_j \\\\\n",
    "sgn \\left( b \\xi_i \\sum_j^N 1 \\right) = \\xi_j \\\\\n",
    "sgn \\left( b N \\xi_i \\right) = \\xi_j\n",
    "\\end{align}\n",
    "\n",
    "Assim:\n",
    "\n",
    "$$W_{ij} = sgn(\\xi_i) = \\xi_j = W_{ij}$$\n",
    "\n",
    "\n",
    "Por isso por conveniencia podemos entao tomar $W_{ij}$ da seguinte maneira:\n",
    "\n",
    "$$\n",
    "W_{ij} = \\begin{cases}\n",
    "\\frac{1}{N} \\xi_i \\xi_j, & i \\ne j \\\\\n",
    "0, &i = j\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Onde $N$ e' o numero de unidades na rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta modelo converge mesmo se um numero (menor do que a metade) dos bits do padrao inicial $S^0$ estiverem erradas, isto e:\n",
    "\n",
    "$$\n",
    "H\\left(S^0, \\xi^{\\mu}\\right) \\le \\left\\lceil \\frac{N}{2} -1 \\right\\rceil\n",
    "$$\n",
    "\n",
    "Sendo $H(\\cdot, \\cdot)$ a norma de Hamming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definimos entao o complementar da norma de Hamming como:\n",
    "\n",
    "$H^\\perp = N - H(\\cdot, \\cdot)$\n",
    "\n",
    "A interpretacao e' o numero de neuronios que estao em desacordo entre dois estados.\n",
    "\n",
    "Podemos tambem definir como:\n",
    "\n",
    "\\begin{align}\n",
    "H       &= \\sum_{\\substack{j=1 \\\\ j\\ne i \\\\ \\xi_j = S_j}}^N \\xi_j S_j \\\\\n",
    "H^\\perp &= \\sum_{\\substack{j=1 \\\\ j\\ne i \\\\ \\xi_j \\ne S_j}}^N \\xi_j S_j \n",
    "\\end{align}\n",
    "\n",
    "Seja a entrada para o i-esimo neuronio $h_i$ dada por:\n",
    "\n",
    "$$\n",
    "h_i = \\sum_{j=1}^N W_{ij} S_j = \\frac{1}{N} \\xi_i \\left( H + H^\\perp \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como por hipotese temos que o numero de elementos coincidentes da norma de Hamming e' maior que a metade do numero de termos entao:\n",
    "\n",
    "$$\n",
    "H + H^\\perp > 1 = c\n",
    "$$\n",
    "\n",
    "Assim sendo:\n",
    "\n",
    "$$\n",
    "h_i = \\frac{c}{N} \\xi_i > 0\n",
    "$$\n",
    "\n",
    "Desta forma os neuronios que estao em concordancia nao alteram conforme o estado do sistema evolua. O padrao e' completamente recuperado, demonstrando que a rede corrige os erros.\n",
    "\n",
    "Por outro lado se tivessemos iniciado com uma quantidade maior que a metade de neuronios diferente, teriamos que todos os neuronios de $S$ e $\\xi$ concordantes trocariam de estado que levaria para convergencia em $-\\xi$ e estabilizaria. Estes padroes sao chamados de **atratores**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplos padroes\n",
    "\n",
    "Para fazermos o sistema lembrar do padrao mais parecido fazemos $W_{ij}$ uma superposicao de todos os termos para cada padrao assim:\n",
    "\n",
    "\\begin{equation}\n",
    "W_{ij} = \\begin{cases}\n",
    "\\frac{1}{N} \\sum_{\\mu=1}^{p} \\xi_i^{\\mu} \\xi_j^{\\mu}, & i \\ne j \\\\\n",
    "0, &i = j\n",
    "\\end{cases},\n",
    "\\;\\;\\forall i,j\n",
    "\\end{equation}\n",
    "\n",
    "onde $p \\ge 1$ e' o numeo de padroes armazenados na rede, nomeado $\\mu$.\n",
    "\n",
    "Esta forma e' conhecida como **Regra de Hebb** da aprendizagem, ou **Regra de Hebb generalizada**. Que diz que os pesos sinapticos mudam em resposta com a experiencia, ou seja ha uma correlacao prporcional entre o disparo dos neuronios pre e pos sinapticos.\n",
    "\n",
    "O modelo matematico vai alem do modelo de **Hebb** pois se nenhum dos dois neuronios ativarem seu peso ira ser positivamente reforcado:\n",
    "\n",
    "$$ \n",
    "\\xi_i^\\mu = \\xi_j^\\mu = -1 \\implies \\xi_i^\\mu \\xi_j^\\mu = 1\n",
    "$$\n",
    "\n",
    "Que provavelmente nao possui correlacao fisiologica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Estabilidade multiplos padroes\n",
    "\n",
    "Seja a condicao geral de estabilidade dada por:\n",
    "\n",
    "\n",
    "$$sgn ( h_i^\\nu ) = \\xi_j \\;\\; \\forall i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "onde a entrada $h_i^\\nu$ para a unidade $i$ no padrao $\\nu$ e dada por:\n",
    "\n",
    "$$h_i^\\nu \\equiv \\sum_{j=1}^N W_{ij} \\xi_j^\\nu = \\frac 1 N  \\left( \\sum_{\\substack{j=1\\\\ j \\ne i}}^N \\sum_{\\mu=1}^p \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu \\right)$$\n",
    "\n",
    "Separando o termo quando $\\mu = \\nu$.\n",
    "\n",
    "$$h_i^\\nu = \\frac{N-1}{N} \\xi_i^\\nu + \\frac 1 N  \\left( \\sum_{\\substack{j=1\\\\ j \\ne i}}^N \\sum_{\\substack{\\mu=1 \\\\ \\mu \\ne \\nu}}^p \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu \\right)$$\n",
    "\n",
    "O segundo termo e' chamado de **termo de diafonia(crosstalk)**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operando a diafonia\n",
    "\n",
    "Vamos definir entao:\n",
    "\n",
    "$$\n",
    "\\sigma_i^\\nu = \\left( \\sum_{\\substack{j=1\\\\ j \\ne i}}^N \\sum_{\\substack{\\mu=1 \\\\ \\mu \\ne \\nu}}^p \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu \\right)\n",
    "$$\n",
    "\n",
    "Desta forma, podemos reescrever a entrada $h_i^\\nu$ como\n",
    "\n",
    "$$h_i^\\nu = \\frac{N-1}{N} \\xi_i^\\nu + \\frac 1 N \\sigma_i^\\nu$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seja entao\n",
    "\n",
    "$$\\frac{1}{N} \\sigma_i^\\nu < \\frac{N-1}{N} \\xi_i^\\nu$$\n",
    "\n",
    "Entao os padroes armazenados sao estaveis. Isto quer dizer que se iniciarmos o sistema com um dos padroes armazenados ele ira convergir. Isto eh cada um dos padroes do sistema sao atratores e o sistema funciona como uma *memoria de conteudo enderecavel*.\n",
    "\n",
    "Por outro lado se:\n",
    "\n",
    "$$\\frac{1}{N} \\sigma_i^\\nu \\ge \\frac{N-1}{N} \\xi_i^\\nu$$\n",
    "\n",
    "Para algum padrao $\\xi^\\mu_i$ de  $\\xi^\\mu$ e' instavel.\n",
    "\n",
    "E' esperado que mais padroes se tornem instaveis na medida em que $p$ cresce. Isto e' quanto mais padroes adicionarmos a rede."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Devaneios\n",
    "\n",
    "Temos:\n",
    "\n",
    "\\begin{align}\n",
    "\\frac{1}{N} \\sigma_i^\\nu &< \\frac{N-1}{N} \\xi_i^\\nu \\\\\n",
    "\\frac{N}{N} \\sigma_i^\\nu &< \\frac{N-1}{1} \\xi_i^\\nu \\\\\n",
    "            \\sigma_i^\\nu &< \\left(N-1\\right)\\xi_i^\\nu \\\\\n",
    "\\end{align}\n",
    "\n",
    "Tambem:\n",
    "\\begin{align}\n",
    "\\sigma_i^\\nu &< \\left(N-1\\right)\\xi_i^\\nu \\\\\n",
    "\\sigma_i^\\nu &< N\\xi_i^\\nu - \\xi_i^\\nu \\\\\n",
    "\\sigma_i^\\nu + \\xi_i^\\nu &< N\\xi_i^\\nu  \\\\\n",
    "\\frac{\\sigma_i^\\nu + \\xi_i^\\nu}{\\xi_i^\\nu} &< N \\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definicao (**Modelo de Hopfield**)\n",
    "\n",
    "E' o modelo de memoria associativa utilizando a regra de Hebb para todos os pares $ij$, com unidades binarias e atualizacao assincrona e' chamado de **Modelo de Hopfield**.\n",
    "\n",
    "Basea-se na ideia de utilizar memorias armazenadas como atratores dinâmicos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capacidade de Armazenamento\n",
    "\n",
    "Seja a equação:\n",
    "\n",
    "\\begin{align}\n",
    "C_i^\\nu \\equiv -\\xi_i^\\nu  \\frac 1 N \\sum_j \\sum_{\\mu \\neq \\nu} \\xi_i^\\mu \\xi_j^\\mu \\xi_j^\\nu\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
